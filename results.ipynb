{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb991af",
   "metadata": {},
   "source": [
    "# Results of the Experiments\n",
    "\n",
    "This notebook contains the results of the models, followed by a section to test the models yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417910d",
   "metadata": {},
   "source": [
    "## SNLI Results\n",
    "\n",
    "This section contains the accuracy scores of the pre-trained models on the SNLI task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ac856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "tokenizer = spacy.load(\"en\")\n",
    "\n",
    "# import required code files\n",
    "from dataset.LoadData import *\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767d81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test dataset\n",
    "vocab, label_vocab, _, _, test_iter = load_snli(device=None, batch_size=64, return_label_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c356a8a",
   "metadata": {},
   "source": [
    "### AWE Model \n",
    "\n",
    "This subsection contains the results of the AWE on the SNLI task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0cc61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82a98a4a9c84cf8b34a3027b985fcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.6793, device='cuda:0'),\n",
      " 'test_loss': tensor(0.7365, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(logger=False,\n",
    "                     checkpoint_callback=False,\n",
    "                     gpus=1 if torch.cuda.is_available() else 0,\n",
    "                     progress_bar_refresh_rate=1)\n",
    "\n",
    "# load the AWE model from the given checkpoint\n",
    "model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/awe/checkpoints/epoch=10.ckpt')\n",
    "\n",
    "# test the model\n",
    "model.freeze()\n",
    "test_result = trainer.test(model, test_dataloaders=test_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83129b51",
   "metadata": {},
   "source": [
    "### UniLSTM Model \n",
    "\n",
    "This subsection contains the results of the unidirectional LSTM on the SNLI task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2faa7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdb45807aff46fc9d086ee3f42c49dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.7971, device='cuda:0'),\n",
      " 'test_loss': tensor(0.5171, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(logger=False,\n",
    "                     checkpoint_callback=False,\n",
    "                     gpus=1 if torch.cuda.is_available() else 0,\n",
    "                     progress_bar_refresh_rate=1)\n",
    "\n",
    "# load the UniLSTM model from the given checkpoint\n",
    "model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/unilstm/checkpoints/epoch=11.ckpt')\n",
    "\n",
    "# test the model\n",
    "model.freeze()\n",
    "test_result = trainer.test(model, test_dataloaders=test_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cbb1c",
   "metadata": {},
   "source": [
    "### BiLSTM Model \n",
    "\n",
    "This subsection contains the results of the bidirectional LSTM on the SNLI task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(logger=False,\n",
    "                     checkpoint_callback=False,\n",
    "                     gpus=1 if torch.cuda.is_available() else 0,\n",
    "                     progress_bar_refresh_rate=1)\n",
    "\n",
    "# load the BiLSTM model from the given checkpoint\n",
    "model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/bilstm/checkpoints/epoch=?.ckpt')\n",
    "\n",
    "# test the model\n",
    "model.freeze()\n",
    "test_result = trainer.test(model, test_dataloaders=test_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3480f",
   "metadata": {},
   "source": [
    "### BiLSTMMax Model \n",
    "\n",
    "This subsection contains the results of the bidirectional LSTM with max pooling on the SNLI task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(logger=False,\n",
    "                     checkpoint_callback=False,\n",
    "                     gpus=1 if torch.cuda.is_available() else 0,\n",
    "                     progress_bar_refresh_rate=1)\n",
    "\n",
    "# load the BiLSTMMax model from the given checkpoint\n",
    "model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/bilstmmax/checkpoints/epoch=?.ckpt')\n",
    "\n",
    "# test the model\n",
    "model.freeze()\n",
    "test_result = trainer.test(model, test_dataloaders=test_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a99e2",
   "metadata": {},
   "source": [
    "## SentEval Results\n",
    "\n",
    "This section contains the scores of the pre-trained models on the SentEval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7475a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6c6dc5",
   "metadata": {},
   "source": [
    "## Test Yourself\n",
    "\n",
    "This section allows you to input your own premise and hypothesis to one of the models and get the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44c07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the input string\n",
    "def tokenize(vocab, sentence):\n",
    "    sentence = tokenizer(sentence)\n",
    "    token_list = []\n",
    "    for token in sentence:\n",
    "        token = token.text.lower()\n",
    "        token_list.append(vocab[token])\n",
    "    return torch.tensor(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3f758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model to use (AWE, UniLSTM, BiLSTM, BiLSTMMax):\n",
      "UniLSTM\n",
      "Input premise:\n",
      "Two women are embracing while holding to go packages.\n",
      "Input hypothesis:\n",
      "Two woman are holding packages.\n",
      "The model predicts the following relation:\n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "# ask the user for the model\n",
    "print('Model to use (AWE, UniLSTM, BiLSTM, BiLSTMMax):')\n",
    "model = input()\n",
    "if (model == 'AWE'):\n",
    "    model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/awe/checkpoints/epoch=10.ckpt')\n",
    "elif (model == 'UniLSTM'):\n",
    "    model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/unilstm/checkpoints/epoch=11.ckpt')\n",
    "elif (model == 'BiLSTM'):\n",
    "    model = model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/bilstm/checkpoints/epoch=?.ckpt')\n",
    "elif (model == 'BiLSTMMax'):\n",
    "    model = FullModel.load_from_checkpoint('pl_logs/lightning_logs/bilstmmax/checkpoints/epoch=?.ckpt')\n",
    "else:\n",
    "    raise Exception('Incorrect model name')\n",
    "\n",
    "# ask user for premise and hypothesis\n",
    "print('Input premise:')\n",
    "premise = input()\n",
    "print('Input hypothesis:')\n",
    "hypothesis = input()\n",
    "\n",
    "# tokenize the premise and hypothesis\n",
    "premise = tokenize(vocab.stoi, premise).unsqueeze(dim=0)\n",
    "hypothesis = tokenize(vocab.stoi, hypothesis).unsqueeze(dim=0)\n",
    "\n",
    "# get the lengths\n",
    "premise_length = torch.tensor([premise.shape[1]])\n",
    "hypothesis_length = torch.tensor([hypothesis.shape[1]])\n",
    "\n",
    "# forward through the embedding\n",
    "premise = model.glove_embeddings(premise)\n",
    "hypothesis = model.glove_embeddings(hypothesis)\n",
    "\n",
    "# forward the premises and hypothesis through the Encoder\n",
    "sentence_representations = model.encoder(premise, premise_length, hypothesis, hypothesis_length)\n",
    "\n",
    "# pass through the classifier\n",
    "predictions = model.classifier(sentence_representations)\n",
    "\n",
    "# get the predicted label\n",
    "predicted_label = torch.argmax(predictions, dim=1)\n",
    "predicted_label = label_vocab.itos[predicted_label]\n",
    "\n",
    "# print the predicted label\n",
    "print('The model predicts the following relation:')\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aef05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
